{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pediatric-florida",
   "metadata": {
    "id": "pediatric-florida"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import regex as re\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "seed = 42\n",
    "\n",
    "\n",
    "\"\"\"Set seed for reproducibility.\"\"\"\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "signal-effect",
   "metadata": {
    "id": "signal-effect"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import torch\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, AutoConfig, AutoModelWithLMHead, AutoTokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-physics",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alpha-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "floppy-commonwealth",
   "metadata": {
    "id": "floppy-commonwealth"
   },
   "outputs": [],
   "source": [
    "train_data, val_data = load_data('./data/train_data.json'), load_data('./data/val_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54P9hwaUSHla",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54P9hwaUSHla",
    "outputId": "9d32e53c-c367-4561-fff2-fed5b3e363e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2882 321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sol_id': 1693719,\n",
       " 'question': 'Rods (peak response at $\\\\lambda=507 \\\\mathrm{~nm}$ ) and cones (peak response at $555 \\\\mathrm{~nm}$ ) are the photosensitive cells in human eye. Although\\n\\nrods are more sensitive, they cannot register colors (unlike cones).\\n\\nGiven that the sensitivity of cone cells is $1 / 220$ of the rod cells, find the threshold values for cone cells.',\n",
       " 'choices': None,\n",
       " 'y_plus_answer': 'The peak response values of rods and cones vary depending on the wavelength of light. \\n\\nRods, which are responsible for vision in low light conditions, have a peak response at around 498 nanometers (blue-green light).\\n\\nCones, which are responsible for color vision and visual acuity in bright light conditions, have three subtypes that respond to different wavelengths of light. The peak response values for these cones are:\\n\\n- S-cones (short-wavelength cones) have a peak response at around 420 nanometers (blue-violet light).\\n- M-cones (medium-wavelength cones) have a peak response at around 534 nanometers (green-yellow light).\\n- L-cones (long-wavelength cones) have a peak response at around 564 nanometers (yellow-orange light).',\n",
       " 'y_minus_answer': 'The sensitivity of cone cells is $1/220$ of the rod cells. This means that for the same amount of light, cones need $220$ times more photons to be activated compared to rods. \\n\\nThe threshold values for rods are typically given as the number of photons needed to activate a rod cell, which is about $100$ photons. Thus, the threshold values for cone cells can be calculated as:\\n\\nThreshold values for cones $= 220 \\\\times 100 =$ 22,000 photons\\n\\nTherefore, for a cone cell to be activated, it needs to receive at least 22,000 photons. This is much higher than the threshold value for rods, which explains why rods are more sensitive to light but cannot register colors..'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_data), len(val_data))\n",
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "optical-going",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "optical-going",
    "outputId": "be4966a2-770d-4f4d-92ba-1712365536e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-belly",
   "metadata": {},
   "source": [
    "### Functions for saving and loading models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "respective-amsterdam",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WEIGHTS_NAME, CONFIG_NAME, OpenAIGPTDoubleHeadsModel, OpenAIGPTTokenizer\n",
    "import os\n",
    "\n",
    "def save_model(name, model, tokenizer, dir_ = 'models/'):\n",
    "\n",
    "    output_dir = \"{}/{}/\".format(dir_, name)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model\n",
    "  \n",
    "    output_model_file = output_dir +  WEIGHTS_NAME\n",
    "    output_config_file = output_dir + CONFIG_NAME\n",
    "\n",
    "    torch.save(model_to_save.state_dict(), output_model_file)\n",
    "    model_to_save.config.to_json_file(output_config_file)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "def load_model(name, dir_ = 'models/'):\n",
    "    output_dir = \"{}/{}/\".format(dir_, name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(output_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-california",
   "metadata": {},
   "source": [
    "### Initialize the main model and tokenizer using T5-small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fluid-sixth",
   "metadata": {
    "id": "fluid-sixth"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model_name = \"t5-small\" \n",
    "\n",
    "sft_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-management",
   "metadata": {},
   "source": [
    "### Upload the Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hungarian-basics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from reward_model import (RewardModelConfig, RewardModel)\n",
    "from transformers import (AutoConfig, AutoModel, AutoTokenizer)\n",
    "\n",
    "path = \"./models/reward_model/models_hf_full/checkpoint-4785/\"\n",
    "\n",
    "AutoConfig.register('RewardModel', RewardModelConfig)\n",
    "AutoModel.register(RewardModelConfig, RewardModel)\n",
    "reward_tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "config = AutoConfig.from_pretrained(path)\n",
    "reward_model = AutoModel.from_pretrained(path, config=config).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-gibson",
   "metadata": {},
   "source": [
    "### Create a Dataset object based on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "stainless-columbus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnswerDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Creating a custom dataset for reading the dataset and\n",
    "    loading it into the dataloader to pass it to the\n",
    "    neural network for finetuning the model\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, data, tokenizer, max_seq_len, answer_name = \"y_plus_answer\"):\n",
    "        \"\"\"\n",
    "        Initializes a Dataset class\n",
    "\n",
    "        Args:\n",
    "            dataframe (pandas.DataFrame): Input dataframe\n",
    "            tokenizer (transformers.tokenizer): Transformers tokenizer\n",
    "            question_max_len (int): Max length of source text\n",
    "            target_len (int): Max length of target text\n",
    "            source_text (str): column name of source text\n",
    "            target_text (str): column name of target text\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokenized_samples = []\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.answer_name = answer_name\n",
    "\n",
    "        for sample in data:\n",
    "            tokenized_sample = self._tokenize(sample)\n",
    "            if len(tokenized_sample['input_ids']) <= self.max_seq_len:\n",
    "                self.tokenized_samples.append(tokenized_sample)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"returns the length of dataframe\"\"\"\n",
    "\n",
    "        return len(self.tokenized_samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"return the input ids, attention masks and target ids\"\"\"\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": self.tokenized_samples[index]['input_ids'].squeeze(),\n",
    "            \"attention_mask\": self.tokenized_samples[index]['attention_mask'].squeeze(),\n",
    "            \"labels\": self.tokenized_samples[index]['labels'].squeeze()\n",
    "        }\n",
    "\n",
    "    def _tokenize(self, sample):\n",
    "        question = \"Answer to the question: \" + sample[\"question\"] + '\\n'\n",
    "        if \"choices\" in sample and sample[\"choices\"] is not None:\n",
    "            question += \"Answer options: \" + \" \".join(f\"{i}) {choice}\" for i, choice in enumerate(sample[\"choices\"]))\n",
    "\n",
    "        model_inputs = self.tokenizer(question, padding='max_length', truncation=True,\n",
    "                                      max_length=self.max_seq_len, return_tensors=\"pt\")\n",
    "\n",
    "        labels = self.tokenizer(text_target = sample[self.answer_name], padding='max_length',\n",
    "                                truncation=True, max_length=self.max_seq_len, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        model_inputs[\"labels\"] = labels\n",
    "\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "_MYXxzrtEoW5",
   "metadata": {
    "id": "_MYXxzrtEoW5"
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH= 512\n",
    "\n",
    "train_dataset = QuestionAnswerDataset(train_data, sft_tokenizer, MAX_SEQ_LENGTH)\n",
    "val_dataset = QuestionAnswerDataset(val_data, sft_tokenizer, MAX_SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fatal-logging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([11801,    12,     8,   822,    10,  8222,     7,    41, 14661,  1773,\n",
       "            44,  1514,     2,    40,   265,   115,    26,     9,  2423,  1752,\n",
       "           940,     3,     2,  3357,   107,    52,    51,     2,    29,    51,\n",
       "             2,  3229,     3,    61,    11, 14075,     7,    41, 14661,  1773,\n",
       "            44,  6422,  3769,     3,     2,  3357,   107,    52,    51,     2,\n",
       "            29,    51,     2,  3229,     3,    61,    33,     8,  1202, 22118,\n",
       "          2640,    16,   936,  1580,     5,  1875,  6102,     7,    33,    72,\n",
       "          6280,     6,    79,  1178,  3691,  2602,    41,   202,  2376, 14075,\n",
       "             7,   137,  9246,    24,     8,     3, 13398,    13, 14075,  2640,\n",
       "            19,  1970,     3,    87,   204,  1755,  3229,    13,     8,  6102,\n",
       "          2640,     6,   253,     8, 12709,  2620,    21, 14075,  2640,     5,\n",
       "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([   37,  6734,  1773,  2620,    13,  6102,     7,    11, 14075,     7,\n",
       "          5215,  3345,    30,     8, 28433,    13,   659,     5,  8222,     7,\n",
       "             6,    84,    33,  1966,    21,  2267,    16,   731,   659,  1124,\n",
       "             6,    43,     3,     9,  6734,  1773,    44,   300,   314,  3916,\n",
       "         13944,  4401,     7,    41,  7060,    15,    18,  9423,   659,   137,\n",
       "          1193,    15,     7,     6,    84,    33,  1966,    21,   945,  2267,\n",
       "            11,  3176,     3,     9,  1071,   485,    16,  2756,   659,  1124,\n",
       "             6,    43,   386,   769,  6137,     7,    24,  3531,    12,   315,\n",
       "         28433,     7,    13,   659,     5,    37,  6734,  1773,  2620,    21,\n",
       "           175, 14075,     7,    33,    10,     3,    18,   180,    18,  1018,\n",
       "            15,     7,    41,     7, 14184,    18, 12301, 19457, 14075,     7,\n",
       "            61,    43,     3,     9,  6734,  1773,    44,   300,     3, 21899,\n",
       "         13944,  4401,     7,    41,  7060,    15,    18, 11275,    15,    17,\n",
       "           659,   137,     3,    18,   283,    18,  1018,    15,     7,    41,\n",
       "          5700,   440,    18, 12301, 19457, 14075,     7,    61,    43,     3,\n",
       "             9,  6734,  1773,    44,   300,   305,  3710, 13944,  4401,     7,\n",
       "            41,  9423,    18,    63,  7126,   210,   659,   137,     3,    18,\n",
       "           301,    18,  1018,    15,     7,    41,  2961,    18, 12301, 19457,\n",
       "         14075,     7,    61,    43,     3,     9,  6734,  1773,    44,   300,\n",
       "           305,  4389, 13944,  4401,     7,    41,    63,  7126,   210,    18,\n",
       "            32,  5517,   659,   137,     1,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100])}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-impression",
   "metadata": {},
   "source": [
    "### Supervised fine-tuning (SFT)\n",
    "\n",
    "We fine-tune T5-small on our labeler demonstrations using supervised learning. We trained for 10 epochs with batch size 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "later-miracle",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = 'models/SFT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aerial-dressing",
   "metadata": {
    "id": "aerial-dressing"
   },
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    dir_ + model_name,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecological-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=sft_tokenizer, model=sft_model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    sft_model,\n",
    "    args,\n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=sft_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "atomic-providence",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2410' max='2410' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2410/2410 32:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.782319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.661401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.218300</td>\n",
       "      <td>2.601440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.218300</td>\n",
       "      <td>2.559311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.816100</td>\n",
       "      <td>2.533958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.816100</td>\n",
       "      <td>2.512206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.734100</td>\n",
       "      <td>2.500043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.734100</td>\n",
       "      <td>2.490313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.688600</td>\n",
       "      <td>2.486020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.688600</td>\n",
       "      <td>2.484408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "save_model(model_name, trainer.model, trainer.tokenizer, dir_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-miracle",
   "metadata": {},
   "source": [
    "Let's check the performance of the SFT-model on a sample of questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "south-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "sft__model, sft_tokenizer = load_model('SFT/t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "conventional-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(model, tokenizer, batch, do_sampling=False):\n",
    "    if not do_sampling:\n",
    "        beam_output = rlhf_trainer.model.generate(\n",
    "                        input_ids=batch['input_ids'].to(rlhf_trainer.model.device),\n",
    "                        attention_mask=batch['attention_mask'].to(rlhf_trainer.model.device),\n",
    "                        num_beams = 5,\n",
    "                        no_repeat_ngram_size = 2, \n",
    "                        early_stopping = True,\n",
    "                        max_length=128)\n",
    "    else:\n",
    "        beam_output = rlhf_trainer.model.generate(\n",
    "                        input_ids=batch['input_ids'].to(rlhf_trainer.model.device),\n",
    "                        attention_mask=batch['attention_mask'].to(rlhf_trainer.model.device),\n",
    "                        do_sample = True, \n",
    "                        no_repeat_ngram_size = 2, \n",
    "                        #top_k = 50, \n",
    "                        top_p = 0.85,\n",
    "                        max_length=128)\n",
    "\n",
    "    for i, output in enumerate(beam_output):\n",
    "        print(\"Question: {}\\n\".format(i) + 100 * '=')\n",
    "        print(rlhf_trainer.tokenizer.decode(batch['input_ids'][i], skip_special_tokens=True))\n",
    "        print(100*'-')\n",
    "        print(\"Output: {}\\n\".format(i))\n",
    "        print(rlhf_trainer.tokenizer.decode(beam_output[i], skip_special_tokens=True))\n",
    "        print(100 * '=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "psychological-freeze",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=True)\n",
    "it = iter(val_loader)\n",
    "batch = next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-prompt",
   "metadata": {},
   "source": [
    "BeamSearch Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "editorial-injury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 0\n",
      "====================================================================================================\n",
      "Answer to the question: Would you describe this learning procedure as reinforcement, supervised or unsupervised learning? Answer options: 0) It is supervised, since we explicitely provide the correct weights to initialize the network. 1) It is unsupervised since the network learns implicit associations present in the input without any additional teaching signal. 2) It is reinforcement learning since only weight updates only occur when a pattern is retrieved correctly.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 0\n",
      "\n",
      "The question asks about this learning procedure as reinforcement, supervised or unsupervised learning. Answer: 0: \"It's not the same thing as a physical learning technique, since we explicitly explicitly provide the correct weights to initialize the network. This is because we explicitely provides the right to Initialize\": False. The answer is false. Therefore, the answer to the question is true.\n",
      "====================================================================================================\n",
      "Question: 1\n",
      "====================================================================================================\n",
      "Answer to the question: Suppose we use the Simplex method to solve the following linear program: beginalign* textbfmaximize hspace0.8cm & hspace0.4cm4x_1 - 6x_2 + 4x_3  textbfsubject tohspace0.6cm & x_1 - 3x_2 + x_3 + s_1 = 1  hspace0.8cm & hspace1.90cmx_1 + s_2 = 8  hspace0.8cm & hspace0.65cm 3x_2 + 2x_3 + s_3 = 6  hspace0.8cm &hspace-0.35cm x_1,: x_2, : x_3, :s_1, :s_2, :s_3 geq 0 endalign* At the current step, we have the following Simplex tableau: beginalign* hspace1cm x_1 &= 1 + 3x_2 - x_3 - s_1  s_2 &= 7 -3x_2 + x_3 + s_1  s_3 &= 6 - 3x_2 - 2x_3  cline1-2 z &= 4 + 6 x_2 - 4s_1 endalign* Write the tableau obtained by executing one iteration (pivot) of the Simplex method starting from the above tableau.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 1\n",
      "\n",
      "The question asks about the Simplex method to solve the following linear program: beginalign*hspace1cm &= 1 + 3x_2 + s_1 + 4x1 = 8 x - X_1,:x2=1x3 =1 w = 0 j =2 m = 2x0$1$2$2. Then, we can use the method of solving the table obtained by executing one iteration (pivot) of the simplex\n",
      "====================================================================================================\n",
      "Question: 2\n",
      "====================================================================================================\n",
      "Answer to the question: Which resolution is improved with CT over projection radiography? Answer options: 0) Temporal resolution 1) Energy resolution 2) Contrast resolution 3) Spatial resolution\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 2\n",
      "\n",
      "The question asks about which resolution is improved with CT over projection radiography. This answer answer is 0: Temporal resolution (CMR) - Contrast resolution, and spatial resolution. It is also a common answer to the question asking about the CT resolution that can be improved by CT.\n",
      "====================================================================================================\n",
      "Question: 3\n",
      "====================================================================================================\n",
      "Answer to the question: In the lecture on bias-variance decomposition we have seen that the true error can be decomposed into noise, bias and variance terms. What happens to the three terms for ridge regression when the regularization parameter $lambda$ grows? Explain your answer. Answer options:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 3\n",
      "\n",
      "The question asks about the true error can be decomposed into noise, bias and variance terms. In the lecture on bias-variance-decomposition, we have seen that the false error is possible in noise. This is because the correct answer is: “The correct answers” are the three terms for ridge regression when the regularization parameter $lambda$ grows. It is not a true answer, but it does not necessarily mean that it will not be true. Therefore, it is true that there is no real answer to this question.\n",
      "====================================================================================================\n",
      "Question: 4\n",
      "====================================================================================================\n",
      "Answer to the question: Consider the equation Answer options: 0) Eq. [mathjaxinline](*)[/mathjaxinline] describes a passive membrane voltage [mathjaxinline]u(t)[/mathjaxinline] driven by spike arrivals. 1) Eq. [mathjaxinline](*)[/mathjaxinline] describes the conductance [mathjaxinline]g(t)[/mathjaxinline] of a simple synapse model. 2) Eq. [mathjaxinline](*)[/mathjaxinline] describes the maximum conductance [mathjaxinline]barg_syn[/mathjaxinline] of a facilitating synapse.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 4\n",
      "\n",
      "The question asks about the equation answer: 1. Eq. [mathjaxinline] describes a passive membrane voltage, driven by spike arrivals. 3. The correct answer to this question is: 2. The facilitating synapse model is based on the assumption that there is no conductance of the underlying molecule, which is not necessarily the case in which it is located in the center of gravity.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "generate_answer(sft_model, sft_tokenizer, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "secure-mountain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 0\n",
      "====================================================================================================\n",
      "Answer to the question: Would you describe this learning procedure as reinforcement, supervised or unsupervised learning? Answer options: 0) It is supervised, since we explicitely provide the correct weights to initialize the network. 1) It is unsupervised since the network learns implicit associations present in the input without any additional teaching signal. 2) It is reinforcement learning since only weight updates only occur when a pattern is retrieved correctly.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 0\n",
      "\n",
      "The correct answer is False. This choice is false, but it is only supervised because the weights to initialize the network are correct. 3) It is coached, since the model of the input is not correctly set by the appropriate weight to start it.\n",
      "====================================================================================================\n",
      "Question: 1\n",
      "====================================================================================================\n",
      "Answer to the question: Suppose we use the Simplex method to solve the following linear program: beginalign* textbfmaximize hspace0.8cm & hspace0.4cm4x_1 - 6x_2 + 4x_3  textbfsubject tohspace0.6cm & x_1 - 3x_2 + x_3 + s_1 = 1  hspace0.8cm & hspace1.90cmx_1 + s_2 = 8  hspace0.8cm & hspace0.65cm 3x_2 + 2x_3 + s_3 = 6  hspace0.8cm &hspace-0.35cm x_1,: x_2, : x_3, :s_1, :s_2, :s_3 geq 0 endalign* At the current step, we have the following Simplex tableau: beginalign* hspace1cm x_1 &= 1 + 3x_2 - x_3 - s_1  s_2 &= 7 -3x_2 + x_3 + s_1  s_3 &= 6 - 3x_2 - 2x_3  cline1-2 z &= 4 + 6 x_2 - 4s_1 endalign* Write the tableau obtained by executing one iteration (pivot) of the Simplex method starting from the above tableau.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 1\n",
      "\n",
      "The question asks about the simplex method to execute the Simplex Method. The following statement is simple: “The simple is for the basic algorithm of a mathematical tool” for solving the system of complex data. Thus, this syntax is called simple. they = h_1, s_2 x & d / t_2. c_4. This means that we have to add +1 + 0 y e - v_3. Let us write -1 + 5 Y X_1 + 2x_3+ * f_\n",
      "====================================================================================================\n",
      "Question: 2\n",
      "====================================================================================================\n",
      "Answer to the question: Which resolution is improved with CT over projection radiography? Answer options: 0) Temporal resolution 1) Energy resolution 2) Contrast resolution 3) Spatial resolution\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 2\n",
      "\n",
      "The question asks about which resolution is improved with CT over projection radiography. The answer is “Quality resolution” which is the resolution of CT to CT. However, the answer does not answer the question. This resolution does NOT interfere with a CT reduction program. Answer #3: “Trehral resolution”. Subscried CT CT (Tretex) to reflect resolutions, they might be effected, and can be an improvement proposition. Option 5: Incorrect resolution, it will result in re-assessment resolution 3) if it is accurate. True. Abstract:\n",
      "====================================================================================================\n",
      "Question: 3\n",
      "====================================================================================================\n",
      "Answer to the question: In the lecture on bias-variance decomposition we have seen that the true error can be decomposed into noise, bias and variance terms. What happens to the three terms for ridge regression when the regularization parameter $lambda$ grows? Explain your answer. Answer options:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 3\n",
      "\n",
      "The question asks about bias-variance decomposition. The answer answer is simply asking about the error as shown in the lecture on bias and variance deposition and also in hearing about its potential. This approach has a positive effect on the fact that the correct answer to the question is incorrect. We can check the true error in bias/variability decomposing and decompute. Answer #3: This is correct, as it can cause some distortion in our lecture, which is why the word \"Fimbda$\" is in its first phase. Since $lamba\" means that no\n",
      "====================================================================================================\n",
      "Question: 4\n",
      "====================================================================================================\n",
      "Answer to the question: Consider the equation Answer options: 0) Eq. [mathjaxinline](*)[/mathjaxinline] describes a passive membrane voltage [mathjaxinline]u(t)[/mathjaxinline] driven by spike arrivals. 1) Eq. [mathjaxinline](*)[/mathjaxinline] describes the conductance [mathjaxinline]g(t)[/mathjaxinline] of a simple synapse model. 2) Eq. [mathjaxinline](*)[/mathjaxinline] describes the maximum conductance [mathjaxinline]barg_syn[/mathjaxinline] of a facilitating synapse.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 4\n",
      "\n",
      "The question asks about the equation. The equation question is asked about Eq. [mathjaxinline](*)[/mathaxine] – a system with ions, which involves the rotor of the cell and the electron electron (sacch & - m) for the diocean in hipponic fields. It is not regulated by reducing the cells of rhoehydethelial activity, as it is used for regulating the relative to synapse. Therefore, the correct answer is \n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "generate_answer(sft_model, sft_tokenizer, batch, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-watershed",
   "metadata": {},
   "source": [
    "### Reinforcement learning (RL). \n",
    "\n",
    "We fine-tuned the SFT model on our environment using REINFORCE: Basics. We optimized \n",
    "$L = L_{MLE} + αL_{RL}$, where $L_{MLE}$ is MLE for groundth true answer to the question and $L_{RL} = -R(Y_{generated})L_{MLE}(Y_{generated})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "orange-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLHFTrainer(Seq2SeqTrainer):\n",
    "    \n",
    "    def add_reward_model(self, reward_model, reward_tokenizer, rl_alpha):\n",
    "        self.reward_model = reward_model\n",
    "        self.reward_tokenizer = reward_tokenizer\n",
    "        self.rl_alpha = rl_alpha\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Compute MLE loss. \n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits') # (batch_size, max_length, vocab_len)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(logits.permute(0, 2, 1), labels) \n",
    "        \n",
    "        # Compute RL loss. \n",
    "        gen_outputs = model.module.generate(\n",
    "            input_ids = inputs['input_ids'].to(self.model.device),\n",
    "            attention_mask = inputs['attention_mask'].to(self.model.device), \n",
    "            do_sample=False,\n",
    "            num_beams=4,\n",
    "            max_length=MAX_SEQ_LENGTH,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True)\n",
    "\n",
    "        batch_size, seq_length = gen_outputs.sequences[:, 1:].shape # skip BOS token\n",
    "        pads = -100*torch.ones((batch_size, MAX_SEQ_LENGTH - seq_length), dtype=torch.long).to(self.model.device)\n",
    "        gen_answers_as_labels = torch.cat((gen_outputs.sequences[:, 1:], pads), dim=1)\n",
    "\n",
    "        gen_input_info = {\n",
    "                'input_ids': inputs['input_ids'].to(self.model.device),\n",
    "                'attention_mask': inputs['attention_mask'].to(self.model.device), \n",
    "                'labels': gen_answers_as_labels.to(self.model.device),\n",
    "                'return_dict': True}\n",
    "        \n",
    "        gen_answer_outputs = model(**gen_input_info)\n",
    "        gen_answer_logits = gen_answer_outputs.get('logits')\n",
    "        rl_criterion = nn.CrossEntropyLoss(reduce=False)\n",
    "        rl_mle = torch.mean(rl_criterion(gen_answer_logits.permute(0, 2, 1), gen_answers_as_labels), dim=-1)\n",
    "        \n",
    "        #Compute rewards\n",
    "        questions = self.tokenizer.batch_decode(inputs['input_ids'], skip_special_tokens=True)\n",
    "        answers = self.tokenizer.batch_decode(gen_outputs.sequences, skip_special_tokens=True)\n",
    "        question_answer_pairs = list(map(lambda i: \"[CLS] \" + questions[i] + \" [SEP] \" + answers[i], np.arange(batch_size)))\n",
    "        reward_inputs = self.reward_tokenizer.batch_encode_plus(question_answer_pairs, padding='max_length', \n",
    "                      truncation=True, max_length=MAX_SEQ_LENGTH, return_tensors=\"pt\")['input_ids']\n",
    "        with torch.no_grad():\n",
    "            rewards = 5 + self.reward_model(reward_inputs.to(self.reward_model.device)).reshape(-1).to(self.model.device)   \n",
    "        \n",
    "        # Mix RL loss with MLE\n",
    "        loss = loss + self.rl_alpha*torch.mean(rl_mle*rewards)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "honest-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "southwest-contribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = 'models/RLHF'\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    dir_,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "frozen-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=sft_tokenizer, model=sft_model.to(device))\n",
    "\n",
    "rlhf_trainer = RLHFTrainer(\n",
    "    sft_model.to(device),\n",
    "    args,\n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=sft_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "contemporary-salmon",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlhf_trainer.add_reward_model(reward_model, reward_tokenizer, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mounted-joining",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2405' max='2405' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2405/2405 5:58:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.471221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.140700</td>\n",
       "      <td>2.420626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.726700</td>\n",
       "      <td>2.391729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.663500</td>\n",
       "      <td>2.379277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.635100</td>\n",
       "      <td>2.374271</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/jagiljazev/mehron_env/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py:32: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 1 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n"
     ]
    }
   ],
   "source": [
    "rlhf_trainer.train()\n",
    "save_model('t5_small', rlhf_trainer.model, rlhf_trainer.tokenizer, dir_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "concerned-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "rlhf_model, rlhf_tokenizer = load_model('RLHF/t5_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "female-theory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 0\n",
      "====================================================================================================\n",
      "Answer to the question: Would you describe this learning procedure as reinforcement, supervised or unsupervised learning? Answer options: 0) It is supervised, since we explicitely provide the correct weights to initialize the network. 1) It is unsupervised since the network learns implicit associations present in the input without any additional teaching signal. 2) It is reinforcement learning since only weight updates only occur when a pattern is retrieved correctly.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 0\n",
      "\n",
      "The question asks about this learning procedure as reinforcement, supervised or unsupervised learning. The answer answer is 0: It is the reinforcement learning, since we explicitly provide the correct weights to initialize the network. Answer 2: \"Imployment learning\": False. This means that a network learns implicit associations present in the input without any additional teaching signal. Therefore, the answer option is not correct.\n",
      "====================================================================================================\n",
      "Question: 1\n",
      "====================================================================================================\n",
      "Answer to the question: Suppose we use the Simplex method to solve the following linear program: beginalign* textbfmaximize hspace0.8cm & hspace0.4cm4x_1 - 6x_2 + 4x_3  textbfsubject tohspace0.6cm & x_1 - 3x_2 + x_3 + s_1 = 1  hspace0.8cm & hspace1.90cmx_1 + s_2 = 8  hspace0.8cm & hspace0.65cm 3x_2 + 2x_3 + s_3 = 6  hspace0.8cm &hspace-0.35cm x_1,: x_2, : x_3, :s_1, :s_2, :s_3 geq 0 endalign* At the current step, we have the following Simplex tableau: beginalign* hspace1cm x_1 &= 1 + 3x_2 - x_3 - s_1  s_2 &= 7 -3x_2 + x_3 + s_1  s_3 &= 6 - 3x_2 - 2x_3  cline1-2 z &= 4 + 6 x_2 - 4s_1 endalign* Write the tableau obtained by executing one iteration (pivot) of the Simplex method starting from the above tableau.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 1\n",
      "\n",
      "The question asks about the following simplex table: beginalign*hspace1cm x_2 - s_1 = 8 p.m. Let's start by executing one iteration (pivot) of the Simplex method. This method is used to solve the linear program by using a method such as 'textbffmaximizing'. Then, we can use the method as an alternative to the simple program. Therefore, the correct answer is: \"\". Suppose we\n",
      "====================================================================================================\n",
      "Question: 2\n",
      "====================================================================================================\n",
      "Answer to the question: Which resolution is improved with CT over projection radiography? Answer options: 0) Temporal resolution 1) Energy resolution 2) Contrast resolution 3) Spatial resolution\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 2\n",
      "\n",
      "The question asks about which resolution is improved with CT over projection radiography. Answer: 0: Temporal resolution (energy resolution) 3) Contrast resolution. The answer: \"Energie-resolution\" : False. It is not a resolution, but it does not improve the resolution of CT. Therefore, the answer is incorrect. This is true, since CT is an effective resolution that can be used to reduce the intensity of the radiation.\n",
      "====================================================================================================\n",
      "Question: 3\n",
      "====================================================================================================\n",
      "Answer to the question: In the lecture on bias-variance decomposition we have seen that the true error can be decomposed into noise, bias and variance terms. What happens to the three terms for ridge regression when the regularization parameter $lambda$ grows? Explain your answer. Answer options:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 3\n",
      "\n",
      "The question asks about the lecture on bias-variance decomposition. This is because the true error can be deposed into noise, bias and variance terms. The correct answer is: $lambda$ is a type of ridge regression variable that is used for regression regression. Using the same algorithm, we can use bias terms to determine the correct error. Therefore, if the regularization parameter increases the value of the variable value, it is less likely to increase the number of variables that are used in regression analysis.\n",
      "====================================================================================================\n",
      "Question: 4\n",
      "====================================================================================================\n",
      "Answer to the question: Consider the equation Answer options: 0) Eq. [mathjaxinline](*)[/mathjaxinline] describes a passive membrane voltage [mathjaxinline]u(t)[/mathjaxinline] driven by spike arrivals. 1) Eq. [mathjaxinline](*)[/mathjaxinline] describes the conductance [mathjaxinline]g(t)[/mathjaxinline] of a simple synapse model. 2) Eq. [mathjaxinline](*)[/mathjaxinline] describes the maximum conductance [mathjaxinline]barg_syn[/mathjaxinline] of a facilitating synapse.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 4\n",
      "\n",
      "The question asks about the equation Answer options. The equation answer options are: 0) Eq. [mathjaxinline](*)]barg_syn. This is because it is a passive membrane voltage that is driven by spike arrivals. Therefore, this is the answer option option, which means that it can be used to describe the conductance of the facilitating sYnapse model. In this case, we need to know that the correct answer choice is correct.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "generate_answer(rlhf_model, rlhf_tokenizer, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "organic-locking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 0\n",
      "====================================================================================================\n",
      "Answer to the question: Would you describe this learning procedure as reinforcement, supervised or unsupervised learning? Answer options: 0) It is supervised, since we explicitely provide the correct weights to initialize the network. 1) It is unsupervised since the network learns implicit associations present in the input without any additional teaching signal. 2) It is reinforcement learning since only weight updates only occur when a pattern is retrieved correctly.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 0\n",
      "\n",
      "Answer #2: It is not supervised, since we explicitly provide the correct weights to initialize the network. The question asks about reinforcement, sag, and unsupervised learning. Answer #3: \"I am a reinforcement learning system\": True. It contains weight-messages, not words, in which translates the information about it. In this case, the number of weight changes can be measured in ten seconds. I am telling you that it is characterized by weight change, which means that the weight of the neural network can only occur when re-ord. Option 4.\n",
      "====================================================================================================\n",
      "Question: 1\n",
      "====================================================================================================\n",
      "Answer to the question: Suppose we use the Simplex method to solve the following linear program: beginalign* textbfmaximize hspace0.8cm & hspace0.4cm4x_1 - 6x_2 + 4x_3  textbfsubject tohspace0.6cm & x_1 - 3x_2 + x_3 + s_1 = 1  hspace0.8cm & hspace1.90cmx_1 + s_2 = 8  hspace0.8cm & hspace0.65cm 3x_2 + 2x_3 + s_3 = 6  hspace0.8cm &hspace-0.35cm x_1,: x_2, : x_3, :s_1, :s_2, :s_3 geq 0 endalign* At the current step, we have the following Simplex tableau: beginalign* hspace1cm x_1 &= 1 + 3x_2 - x_3 - s_1  s_2 &= 7 -3x_2 + x_3 + s_1  s_3 &= 6 - 3x_2 - 2x_3  cline1-2 z &= 4 + 6 x_2 - 4s_1 endalign* Write the tableau obtained by executing one iteration (pivot) of the Simplex method starting from the above tableau.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 1\n",
      "\n",
      "The Simplex will be able to use this function as a tool to provide gnalign**. This will support the simplex method that will generate nt, but instead create's' by adding the following function. The simpleix method is the simplest method to perform simple - in tween, and is converted into y_2 and then is changed. In this case, the Simpleix is based on the syntax of the base for the initial statement. It also provides an overview of simple information in the tableau and solve. There\n",
      "====================================================================================================\n",
      "Question: 2\n",
      "====================================================================================================\n",
      "Answer to the question: Which resolution is improved with CT over projection radiography? Answer options: 0) Temporal resolution 1) Energy resolution 2) Contrast resolution 3) Spatial resolution\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 2\n",
      "\n",
      "The question asks about the resolution of CT over projection radiography. The questions ask about which resolution is improved with CTover projection radiation. This answer is asking about what CTg is all about. CT is a resolution improvement measure. Currently, it does not have any limitations. However, CT uses the CT overlay imaging system to visualize the effects of anMRI technique, but it is often used to compare these effects with other imaging devices. Answer #2: \"Contrast resolution\": True. It depends on the state of atoms, not the method for each image to enhance the surface of the body over-\n",
      "====================================================================================================\n",
      "Question: 3\n",
      "====================================================================================================\n",
      "Answer to the question: In the lecture on bias-variance decomposition we have seen that the true error can be decomposed into noise, bias and variance terms. What happens to the three terms for ridge regression when the regularization parameter $lambda$ grows? Explain your answer. Answer options:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 3\n",
      "\n",
      "The correct answer is \"Worth.\" : The answer: True error can be decomposed into noise, bias and variance terms. Answer: False error in the first word is that the correct spelling will be not the exact correct or correct. The second word, $lambda$ is a string of other arguments that can make sense by deleting errors and parallainting the word and renaming the error. Finally, the correction is possible by explaining the difference between the term and the probability. This is not correct, and will not appear on the subject of the three\n",
      "====================================================================================================\n",
      "Question: 4\n",
      "====================================================================================================\n",
      "Answer to the question: Consider the equation Answer options: 0) Eq. [mathjaxinline](*)[/mathjaxinline] describes a passive membrane voltage [mathjaxinline]u(t)[/mathjaxinline] driven by spike arrivals. 1) Eq. [mathjaxinline](*)[/mathjaxinline] describes the conductance [mathjaxinline]g(t)[/mathjaxinline] of a simple synapse model. 2) Eq. [mathjaxinline](*)[/mathjaxinline] describes the maximum conductance [mathjaxinline]barg_syn[/mathjaxinline] of a facilitating synapse.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Output: 4\n",
      "\n",
      "The question asks about the equation. The answer is a question: The term Eq. [mathjaxinline](*)[/mathaxine] is used to describe the conductance [aspheric] mr. It is the term f which is applied to the gt tins of the membrane voltage. This means that synapses are not confined to their membrane systems. Therefore, this means insertion of ct = barg_sn[/ather]. This equation is incorrect.\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "generate_answer(rlhf_model, rlhf_tokenizer, batch, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "white-weekend",
   "metadata": {},
   "source": [
    "### Evaluation using a Reward Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-aircraft",
   "metadata": {},
   "source": [
    "Function for generating answers to the questions asked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "occasional-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(model, tokenizer, questions):\n",
    "    MAX_SEQ_LENGTH= 512\n",
    "\n",
    "    generated_answers = []\n",
    "\n",
    "    for sample in questions:\n",
    "        question = \"Answer to the question: \" + sample[\"question\"] + '\\n'\n",
    "        if \"choices\" in sample and sample[\"choices\"] is not None:\n",
    "            question += \"Answer options: \" + \" \".join(\"{}) {}\".format(i, choice) for i, choice in enumerate(sample[\"choices\"]))\n",
    "\n",
    "        model_inputs = tokenizer(question, padding='max_length', truncation=True,\n",
    "                                      max_length=MAX_SEQ_LENGTH, return_tensors=\"pt\")\n",
    "\n",
    "        beam_output = model.generate(\n",
    "                        input_ids=model_inputs['input_ids'].to(model.device),\n",
    "                        attention_mask=model_inputs['attention_mask'].to(model.device),\n",
    "                        num_beams = 5,\n",
    "                        no_repeat_ngram_size = 2, \n",
    "                        early_stopping = True,\n",
    "                        max_length=256)\n",
    "\n",
    "        sample['generated_answer'] = tokenizer.batch_decode(beam_output, skip_special_tokens=True)[0]\n",
    "        generated_answers.append(sample)\n",
    "        \n",
    "    return generated_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-investigation",
   "metadata": {},
   "source": [
    "Function for preparing input data for the reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "interim-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_input(data):\n",
    "    reward_input = []\n",
    "    for sample in data:\n",
    "        question = sample[\"question\"]\n",
    "        if \"choices\" in sample and sample[\"choices\"] is not None:\n",
    "            question += \" \".join(f\"{i}) {choice}\" for i, choice in enumerate(sample[\"choices\"]))\n",
    "        input_ids = reward_tokenizer.encode(\"[CLS] \" + question + \" [SEP] \" + sample['generated_answer'], \n",
    "                        padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")    \n",
    "        reward_input.append(input_ids)\n",
    "        \n",
    "    return torch.cat(reward_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-dictionary",
   "metadata": {},
   "source": [
    "Function for calculating the average reward for generated responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "going-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_reward(reward_input, model_name='SFT'):\n",
    "    batch_size = 4\n",
    "    rewards = []\n",
    "    for i in range(0, len(reward_input), batch_size):\n",
    "        batch = reward_input[i:i+batch_size].to(reward_model.device)\n",
    "        rewards += list(reward_model(batch).detach().cpu().numpy())\n",
    "    print('Model: ', model_name)\n",
    "    print('Number of samples: ', len(rewards))\n",
    "    print('Mean Reward: ', np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "speaking-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = load_data('prompts.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-criticism",
   "metadata": {},
   "source": [
    "### SFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "gentle-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "sft_model, sft_tokenizer = load_model('SFT/t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "charming-cattle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  SFT\n",
      "Number of samples:  100\n",
      "Mean Reward:  1.4191724\n"
     ]
    }
   ],
   "source": [
    "generated_answers = generate_answers(sft_model, sft_tokenizer, questions)\n",
    "reward_input = format_reward_input(generated_answers)\n",
    "calculate_avg_reward(reward_input, model_name='SFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-coordination",
   "metadata": {},
   "source": [
    "### Final RLHF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "outstanding-longer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "rlhf_model, rlhf_tokenizer = load_model('RLHF/t5_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "south-slovakia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  RLHF\n",
      "Number of samples:  100\n",
      "Mean Reward:  1.8264363\n"
     ]
    }
   ],
   "source": [
    "generated_answers = generate_answers(rlhf_model, rlhf_tokenizer, questions)\n",
    "reward_input = format_reward_input(generated_answers)\n",
    "calculate_avg_reward(reward_input, model_name='RLHF')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mehron_env",
   "language": "python",
   "name": "mehron_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01466d9fd81b4f799813d730e8a1e217": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "109442f598484cb297e244c50e6b6690": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_cb06b0b28813473d8dcb317977da9122",
      "style": "IPY_MODEL_61124e1a799b49ed855bbaf587399628",
      "tooltip": ""
     }
    },
    "2d659544d9e74f5685eca59fec2f2e93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_935d8cc7a0f3494787e0ea830f1d69fd",
       "IPY_MODEL_ed52c1ad85504bd29fd83788844fc0b3",
       "IPY_MODEL_78ab5cbc924f4c5d954857505160ae13",
       "IPY_MODEL_cdb5949baefa4aae99e76fe8543566e5"
      ],
      "layout": "IPY_MODEL_b9f41001feb24938aba8ddeb69c147f7"
     }
    },
    "31fe22420880468ba1342ea411818e1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "418b015bc92b47268925d537c8e88281": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5048e5186ebf4357bc40084e893bd8ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61124e1a799b49ed855bbaf587399628": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "61e5e261c8b0474581c4cc404a1ab9ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78ab5cbc924f4c5d954857505160ae13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d13f9224e3d24922a877cf2ee2e889ca",
      "placeholder": "​",
      "style": "IPY_MODEL_7bca490d6f4a4466965dec95c4ed2f9c",
      "value": "Your token has been saved to /root/.cache/huggingface/token"
     }
    },
    "7b669659d7514a5fad1db6db30d111d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89e9530dc4e04866a8deccba0f7f5da4",
      "placeholder": "​",
      "style": "IPY_MODEL_c37ba1babfb44d6aa5201d4811d1b58e",
      "value": "Connecting..."
     }
    },
    "7bca490d6f4a4466965dec95c4ed2f9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8596c50991294c5ab02b8ced203997f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "CheckboxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "CheckboxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "CheckboxView",
      "description": "Add token as git credential?",
      "description_tooltip": null,
      "disabled": false,
      "indent": true,
      "layout": "IPY_MODEL_31fe22420880468ba1342ea411818e1d",
      "style": "IPY_MODEL_a7d3130d2e9446b4ad927c01d4d3de9b",
      "value": true
     }
    },
    "89e9530dc4e04866a8deccba0f7f5da4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "935d8cc7a0f3494787e0ea830f1d69fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7b9fa2aa6ff4b3ab95b772c0bb8c0a6",
      "placeholder": "​",
      "style": "IPY_MODEL_c54ea6347f2c4f709b8b63e5098e90ec",
      "value": "Token is valid (permission: read)."
     }
    },
    "9b9a5b7a07144e7d99c3e00fe14936e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7d3130d2e9446b4ad927c01d4d3de9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aaf16ce1d9874c888206ff6aba657e24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5ab991ddd754211a53031567c9c9000": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9f41001feb24938aba8ddeb69c147f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "ba97449484fe445488c9415f2452a75e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5ab991ddd754211a53031567c9c9000",
      "placeholder": "​",
      "style": "IPY_MODEL_e04178c238f94045bd3078972615b039",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
     }
    },
    "c1daf699736a4b91b42477e547e413cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c37ba1babfb44d6aa5201d4811d1b58e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c54ea6347f2c4f709b8b63e5098e90ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c7b9fa2aa6ff4b3ab95b772c0bb8c0a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb06b0b28813473d8dcb317977da9122": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdb5949baefa4aae99e76fe8543566e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5048e5186ebf4357bc40084e893bd8ff",
      "placeholder": "​",
      "style": "IPY_MODEL_aaf16ce1d9874c888206ff6aba657e24",
      "value": "Login successful"
     }
    },
    "d0f4b33f4dec435bb5e4d132952dd6a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61e5e261c8b0474581c4cc404a1ab9ef",
      "placeholder": "​",
      "style": "IPY_MODEL_01466d9fd81b4f799813d730e8a1e217",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "d13f9224e3d24922a877cf2ee2e889ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d760ff8f19694139b0a70a596b05fadb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e04178c238f94045bd3078972615b039": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ed52c1ad85504bd29fd83788844fc0b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d760ff8f19694139b0a70a596b05fadb",
      "placeholder": "​",
      "style": "IPY_MODEL_9b9a5b7a07144e7d99c3e00fe14936e5",
      "value": "Your token has been saved in your configured git credential helpers (store)."
     }
    },
    "f3b2ed8fe8d749aa92f59fe8e0447473": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_418b015bc92b47268925d537c8e88281",
      "placeholder": "​",
      "style": "IPY_MODEL_c1daf699736a4b91b42477e547e413cc",
      "value": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
